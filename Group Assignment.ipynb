{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c18fd1db-2305-4b68-ab09-eac4335dfca7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Recognizing Brand & Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e9514d7-d53a-464b-8d35-084f4f6231a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "475860df-d4cf-49ed-9b0a-1204ad9ccd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pd.read_csv('models.csv', header = None, names = ['Brand', 'Model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f28abea-7a41-46c9-aebf-4bb02f04fc46",
   "metadata": {},
   "source": [
    "**Delete Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e615b1e4-3964-4968-aef3-ae51e25988ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "models['Brand'] = models['Brand'].str.lower()\n",
    "models['Model'] = models['Model'].str.lower()\n",
    "models['Combined'] = (models['Brand'] + ' ' + models['Model'])\n",
    "models['Combined'] = models['Combined'].str.replace('[\\s\\-,._]', '', regex = True)\n",
    "\n",
    "unique_model = models.drop_duplicates(subset = 'Combined')\n",
    "unique_model = unique_model.drop(columns = ['Combined'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c774c40-aa8f-4863-bf7c-b09f4446084c",
   "metadata": {},
   "source": [
    "**Check Brand list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "545c383d-2d42-494a-ac6b-56a66f4852f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acura', 'audi', 'bmw', 'buick', 'cadillac', 'car', 'chevrolet', 'chrysler', 'dodge', 'ford', 'honda', 'hyndai kia', 'hyundai', 'hyundai,', 'infiniti', 'kia', 'lincoln', 'mazda', 'mercedes', 'mercury', 'mitsubishi', 'nissan', 'nissan.', 'pontiac', 'problem', 'saturn', 'seat', 'sedan', 'subaru', 'suzuki', 'toyata', 'toyota', 'volkswagen', 'volkwagen', 'volvo']\n"
     ]
    }
   ],
   "source": [
    "brand = unique_model['Brand'].unique().tolist()\n",
    "print(brand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a4c08f-d2c6-48a8-98cf-0b489663dffd",
   "metadata": {},
   "source": [
    "**Notice weird brands:**\n",
    "\n",
    "- _hyundai kia_, _car_, _problem_, _seat_ should be deleted\n",
    "- _hyndai kia_ should be _hyundai_\n",
    "- _hyundai,_ and _nissan._ should not have ',' and '.'.\n",
    "- _toyata_ should be _toyota_\n",
    "- _volkwagen_ should be _volkswagen_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df20ccd8-febd-4581-bcfc-9792e9a260bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z4/djkh1_vd3bb6xf3hy80b_l9m0000gn/T/ipykernel_24317/1040335118.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_model.loc[new_model['Brand'].str.contains('hyundai') & ~ new_model['Brand'].eq('hyundai'), 'Brand'] = 'hyundai'\n",
      "/var/folders/z4/djkh1_vd3bb6xf3hy80b_l9m0000gn/T/ipykernel_24317/1040335118.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_model.loc[new_model['Brand'].str.contains('kia') & ~ new_model['Brand'].eq('kia'), 'Brand'] = 'kia'\n",
      "/var/folders/z4/djkh1_vd3bb6xf3hy80b_l9m0000gn/T/ipykernel_24317/1040335118.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_model.loc[new_model['Brand'].str.contains('nissan') & ~ new_model['Brand'].eq('nissan'), 'Brand'] = 'nissan'\n",
      "/var/folders/z4/djkh1_vd3bb6xf3hy80b_l9m0000gn/T/ipykernel_24317/1040335118.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_model.loc[new_model['Brand'].eq('toyata'), 'Brand'] = 'toyota'\n",
      "/var/folders/z4/djkh1_vd3bb6xf3hy80b_l9m0000gn/T/ipykernel_24317/1040335118.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_model.loc[new_model['Brand'].eq('volkwagen'), 'Brand'] = 'volkswagen'\n"
     ]
    }
   ],
   "source": [
    "new_model = unique_model[~unique_model['Brand'].isin(['hyundai kia', 'car', 'problem', 'seat'])]\n",
    "\n",
    "new_model.loc[new_model['Brand'].str.contains('hyundai') & ~ new_model['Brand'].eq('hyundai'), 'Brand'] = 'hyundai'\n",
    "new_model.loc[new_model['Brand'].str.contains('kia') & ~ new_model['Brand'].eq('kia'), 'Brand'] = 'kia'\n",
    "new_model.loc[new_model['Brand'].str.contains('nissan') & ~ new_model['Brand'].eq('nissan'), 'Brand'] = 'nissan'\n",
    "new_model.loc[new_model['Brand'].eq('toyata'), 'Brand'] = 'toyota'\n",
    "new_model.loc[new_model['Brand'].eq('volkwagen'), 'Brand'] = 'volkswagen'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f07542-f156-48ae-81b1-747b227f1c57",
   "metadata": {},
   "source": [
    "**Delete Dup Again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3338c842-245e-49b3-81d8-528cd5736aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z4/djkh1_vd3bb6xf3hy80b_l9m0000gn/T/ipykernel_24317/354213253.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_model['Combined'] = (new_model['Brand'] + ' ' + new_model['Model'])\n",
      "/var/folders/z4/djkh1_vd3bb6xf3hy80b_l9m0000gn/T/ipykernel_24317/354213253.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_model['Combined'] = new_model['Combined'].str.replace('[\\s\\-,._]', '', regex = True)\n"
     ]
    }
   ],
   "source": [
    "new_model['Combined'] = (new_model['Brand'] + ' ' + new_model['Model'])\n",
    "new_model['Combined'] = new_model['Combined'].str.replace('[\\s\\-,._]', '', regex = True)\n",
    "\n",
    "new_model = new_model.drop_duplicates(subset = 'Combined')\n",
    "new_model = new_model.drop(columns = ['Combined'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81369f82-1e55-40e0-a12b-cd518a6a9b84",
   "metadata": {},
   "source": [
    "**Check brand**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5fbff63-f077-4121-99ab-136baf843f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acura', 'audi', 'bmw', 'buick', 'cadillac', 'chevrolet', 'chrysler', 'dodge', 'ford', 'honda', 'kia', 'hyundai', 'infiniti', 'lincoln', 'mazda', 'mercedes', 'mercury', 'mitsubishi', 'nissan', 'pontiac', 'saturn', 'sedan', 'subaru', 'suzuki', 'toyota', 'volkswagen', 'volvo']\n"
     ]
    }
   ],
   "source": [
    "brand = new_model['Brand'].unique().tolist()\n",
    "print(brand)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f68eeb5-e3b3-45aa-b5fb-880364414d4a",
   "metadata": {},
   "source": [
    "**Check model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e510a5a-0d7c-4671-926e-7620a80a7035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481\n",
      "478\n",
      "Rows with duplicate values in 'Model' column:\n",
      "       Brand    Model\n",
      "1      acura   legend\n",
      "44     buick  century\n",
      "171    honda   legend\n",
      "224  hyundai   matrix\n",
      "420   toyota  century\n",
      "444   toyota   matrix\n"
     ]
    }
   ],
   "source": [
    "print(len(new_model))\n",
    "print(new_model['Model'].nunique())\n",
    "duplicated_model = new_model[new_model.duplicated(subset = 'Model', keep = False)]\n",
    "print(\"Rows with duplicate values in 'Model' column:\")\n",
    "print(duplicated_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0fc709-a389-4a74-b1d7-6ab309d4cb25",
   "metadata": {},
   "source": [
    "**Modify model dup & create dictionary for models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "873c5476-c985-41a3-8f13-84d19407b7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481\n"
     ]
    }
   ],
   "source": [
    "modify_rows = [1,44,171, 224, 420, 444]\n",
    "new_model.loc[modify_rows, 'Model'] = new_model.loc[modify_rows, 'Brand'] + ' ' + new_model.loc[modify_rows, 'Model']\n",
    "print(new_model['Model'].nunique())\n",
    "model = new_model.set_index('Model')['Brand'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f7782-b02a-48f7-9c23-4d8ec0430803",
   "metadata": {},
   "source": [
    "# Scrape Comments & Create CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f1eff7-6e09-46f6-ae4f-23e2f17f3079",
   "metadata": {},
   "source": [
    "**Note:** We still need to find the URL first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a3ca00-1711-4ba7-8c50-66d7ad9fd439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "entries = []\n",
    "entry = []\n",
    "urlnumber = 1\n",
    "\n",
    "while urlnumber < 101:\n",
    "    url = f'https://forums.edmunds.com/discussion/2864/general/x/entry-level-luxury-performance-sedans/p{urlnumber}'\n",
    "    try:\n",
    "        r = requests.get(url, timeout = 10) # Sending a request to access the page\n",
    "    except Exception as e:\n",
    "        print(\"Error message:\",e)\n",
    "        break;\n",
    "\n",
    "    data = r.text\n",
    "    \n",
    "    soup = BeautifulSoup(data, 'lxml') # Getting the page source into the soup\n",
    "    \n",
    "    for div in soup.find_all('div'):\n",
    "        entry = []\n",
    "        if(div.get('class') != None and div.get('class')[0] == 'Comment'): # A single post is referred to as a comment. Each comment is a block denoted in a div tag which has a class called comment.\n",
    "            ps = div.find_all('p') # gets all the tags called p to a variable ps\n",
    "            aas = div.find_all('a') # gets all the tags called a to a variable aas\n",
    "            spans = div.find_all('span')\n",
    "            times = div.find_all('time') # used to extract the time tag which gives the iDate of the post\n",
    "\n",
    "            concat_str = ''\n",
    "            for str in aas[1].contents: # prints the contents that is between the tag start and end\n",
    "                if str != \"<br>\" or str != \"<br/>\": # breaks in post which we need to work around\n",
    "                    concat_str = (concat_str + ' '+ str).encode(\"utf-8\").strip() # the format extracted is a unicode - we need a uniform structure to work with the strings\n",
    "            entry.append(concat_str)\n",
    "\n",
    "            concat_str = ''\n",
    "            for str in times[0].contents:\n",
    "                if str != \"<br>\" or str != \"<br/>\":\n",
    "                    concat_str = (concat_str + ' '+ str).encode('iso-8859-1').strip()\n",
    "            entry.append(concat_str)\n",
    "\n",
    "            for div in div.find_all('div'):\n",
    "                if (div.get('class') != None and div.get('class')[0] == 'Message'): # extracting the div tag with the class attribute as message\n",
    "                    blockquotes = []\n",
    "                    x = div.get_text()\n",
    "                    for bl in div.find_all('blockquote'):\n",
    "                        blockquotes.append(bl.get_text()) # block quote is used to get the quote made by a person. get_text helps to eliminate the hyperlinks and pulls out only the data.\n",
    "                        bl.decompose()\n",
    "                    # Encoding the text to ascii code by replacing the non-ascii characters\n",
    "                    ascii_encoding = div.get_text().replace(\"\\n\",\" \").replace(\"<br/>\",\"\").encode('ascii','replace')\n",
    "                    # Convert the ASCII encoding to Latin1 encoding\n",
    "                    latin1_encoding = ascii_encoding.decode('ascii').encode('iso-8859-1')\n",
    "                    # Append the encoding bytes to output list\n",
    "                    entry.append(latin1_encoding)\n",
    "\n",
    "                    for bl in blockquotes:\n",
    "                        ascii_encoding = bl.replace(\"\\n\",\" \").replace(\"<br/>\",\"\").encode('ascii','replace')\n",
    "                        latin1_encoding = ascii_encoding.decode('ascii').encode('iso-8859-1')\n",
    "                        entry.append(latin1_encoding)\n",
    "\n",
    "            entries.append(entry)\n",
    "            \n",
    "    urlnumber += 1\n",
    "\n",
    "columns = ['User Name', 'Comment Date', 'Full Comment']\n",
    "\n",
    "# Convert a list of byte to list a of string     \n",
    "stringlist=[[x.decode('iso-8859-1') for x in entry] for entry in entries]\n",
    "# Save the list to a csv file\n",
    "with open('5KComments.csv', 'w') as output:\n",
    "    writer = csv.writer(output, quoting=csv.QUOTE_ALL)\n",
    "    writer.writerow(columns)\n",
    "    writer.writerows(stringlist)\n",
    "\n",
    "print (\"Wrote to 5KComments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a85d962-0833-4ed5-a175-b26af577d782",
   "metadata": {},
   "source": [
    "# To Do 1: Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9a3565-11b9-4fc8-8f98-b48223fb29b6",
   "metadata": {},
   "source": [
    "# To Do 2: Find word similarity\n",
    "\n",
    "**Question:** Should we focus only on those comments when they spelled the brand/model 100% correct? If so, we can ignore this step. If not, we need to determine whether they are having typos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b350aaa-556d-4cde-9d1f-1171fa263274",
   "metadata": {},
   "source": [
    "### Approach 1 - Simpler, but may have lower accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56f215e4-77d2-4ff9-a381-1bb2f1ad62b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity: 66.67%\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import spacy.cli\n",
    "import en_core_web_sm\n",
    "\n",
    "def jaccard_similarity(word1, word2):\n",
    "    set1 = set(word1)\n",
    "    set2 = set(word2)\n",
    "\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "\n",
    "    similarity = intersection / union\n",
    "    return similarity\n",
    "\n",
    "word1 = \"Honda\"\n",
    "word2 = \"honda\"\n",
    "\n",
    "similarity_percentage = jaccard_similarity(word1, word2) * 100\n",
    "print(f\"Jaccard Similarity: {similarity_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2828dccd-26a7-40a9-933b-850d0ad91f2d",
   "metadata": {},
   "source": [
    "### Approach 2 - Have higher accuracy, but hard to determine the similarity threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "406cd1ba-f983-4268-a86c-9b73a75eb301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Similarity: 70.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z4/djkh1_vd3bb6xf3hy80b_l9m0000gn/T/ipykernel_24317/147863855.py:11: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  similarity = doc1.similarity(doc2)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import spacy.cli\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "def word_similarity(word1, word2):\n",
    "    doc1 = nlp(word1)\n",
    "    doc2 = nlp(word2)\n",
    "\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    return similarity\n",
    "\n",
    "word1 = \"Honda\"\n",
    "word2 = \"honda\"\n",
    "\n",
    "similarity_percentage = word_similarity(word1, word2) * 100\n",
    "print(f\"Word Similarity: {similarity_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4507a177-8399-4814-88f6-f80da7dbd12b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5371f84f-435a-457f-bca9-1b06f9644fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d8a787-36de-442d-b43b-4c8f722939ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
