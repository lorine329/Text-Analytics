{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d93a75d5-0f3d-4e4b-a716-5922da5cba6d",
   "metadata": {},
   "source": [
    "# Scrape Comments & Create CSV\n",
    "\n",
    "URL: https://forums.edmunds.com/discussion/8/general/x/car-commercials-the-good-the-bad-and-the-annoying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86864e1-372a-422d-b057-fec8caa97a51",
   "metadata": {},
   "source": [
    "### 5K comment only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3903412-1b0a-4f75-98b5-18a88c8a3cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "entries = []\n",
    "entry = []\n",
    "urlnumber = 1\n",
    "\n",
    "while urlnumber < 101:\n",
    "    url = f'https://forums.edmunds.com/discussion/8/general/x/car-commercials-the-good-the-bad-and-the-annoying/p{urlnumber}'\n",
    "    try:\n",
    "        r = requests.get(url, timeout = 10) # Sending a request to access the page\n",
    "    except Exception as e:\n",
    "        print(\"Error message:\",e)\n",
    "        break;\n",
    "\n",
    "    data = r.text\n",
    "    \n",
    "    soup = BeautifulSoup(data, 'lxml') # Getting the page source into the soup\n",
    "    \n",
    "    for div in soup.find_all('div'):\n",
    "        entry = []\n",
    "        if(div.get('class') != None and div.get('class')[0] == 'Comment'): # A single post is referred to as a comment. Each comment is a block denoted in a div tag which has a class called comment.\n",
    "            ps = div.find_all('p') # gets all the tags called p to a variable ps\n",
    "            aas = div.find_all('a') # gets all the tags called a to a variable aas\n",
    "            spans = div.find_all('span')\n",
    "            times = div.find_all('time') # used to extract the time tag which gives the iDate of the post\n",
    "\n",
    "            concat_str = ''\n",
    "            for str in aas[1].contents: # prints the contents that is between the tag start and end\n",
    "                if str != \"<br>\" or str != \"<br/>\": # breaks in post which we need to work around\n",
    "                    concat_str = (concat_str + ' '+ str).encode(\"utf-8\").strip() # the format extracted is a unicode - we need a uniform structure to work with the strings\n",
    "            entry.append(concat_str)\n",
    "\n",
    "            concat_str = ''\n",
    "            for str in times[0].contents:\n",
    "                if str != \"<br>\" or str != \"<br/>\":\n",
    "                    concat_str = (concat_str + ' '+ str).encode('iso-8859-1').strip()\n",
    "            entry.append(concat_str)\n",
    "\n",
    "            for div in div.find_all('div'):\n",
    "                if (div.get('class') != None and div.get('class')[0] == 'Message'): # extracting the div tag with the class attribute as message\n",
    "                    blockquotes = []\n",
    "                    x = div.get_text()\n",
    "                    for bl in div.find_all('blockquote'):\n",
    "                        blockquotes.append(bl.get_text()) # block quote is used to get the quote made by a person. get_text helps to eliminate the hyperlinks and pulls out only the data.\n",
    "                        bl.decompose()\n",
    "                    # Encoding the text to ascii code by replacing the non-ascii characters\n",
    "                    ascii_encoding = div.get_text().replace(\"\\n\",\" \").replace(\"<br/>\",\"\").encode('ascii','replace')\n",
    "                    # Convert the ASCII encoding to Latin1 encoding\n",
    "                    latin1_encoding = ascii_encoding.decode('ascii').encode('iso-8859-1')\n",
    "                    # Append the encoding bytes to output list\n",
    "                    entry.append(latin1_encoding)\n",
    "\n",
    "                    for bl in blockquotes:\n",
    "                        ascii_encoding = bl.replace(\"\\n\",\" \").replace(\"<br/>\",\"\").encode('ascii','replace')\n",
    "                        latin1_encoding = ascii_encoding.decode('ascii').encode('iso-8859-1')\n",
    "                        entry.append(latin1_encoding)\n",
    "\n",
    "            entries.append(entry)\n",
    "            \n",
    "    urlnumber += 1\n",
    "\n",
    "columns = ['User Name', 'Comment Date', 'Full Comment']\n",
    "\n",
    "# Convert a list of byte to list a of string     \n",
    "stringlist=[[x.decode('iso-8859-1') for x in entry] for entry in entries]\n",
    "# Save the list to a csv file\n",
    "with open('5KComments.csv', 'w') as output:\n",
    "    writer = csv.writer(output, quoting=csv.QUOTE_ALL)\n",
    "    writer.writerow(columns)\n",
    "    writer.writerows(stringlist)\n",
    "\n",
    "print (\"Wrote to 5KComments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0839099-0ee2-4629-9f7b-aa1e29f406a9",
   "metadata": {},
   "source": [
    "### All Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea78749-9efc-4eb9-9ed3-4571b2aec66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "entries = []\n",
    "entry = []\n",
    "urlnumber = 1\n",
    "\n",
    "while urlnumber < 168:\n",
    "    url = f'https://forums.edmunds.com/discussion/8/general/x/car-commercials-the-good-the-bad-and-the-annoying/p{urlnumber}'\n",
    "    try:\n",
    "        r = requests.get(url, timeout = 10) # Sending a request to access the page\n",
    "    except Exception as e:\n",
    "        print(\"Error message:\",e)\n",
    "        break;\n",
    "\n",
    "    data = r.text\n",
    "    \n",
    "    soup = BeautifulSoup(data, 'lxml') # Getting the page source into the soup\n",
    "    \n",
    "    for div in soup.find_all('div'):\n",
    "        entry = []\n",
    "    \n",
    "        try:\n",
    "            class_attribute = div.get('class')\n",
    "            if class_attribute is not None and class_attribute[0] == 'Comment':\n",
    "                ps = div.find_all('p')\n",
    "                aas = div.find_all('a')\n",
    "                spans = div.find_all('span')\n",
    "                times = div.find_all('time')\n",
    "\n",
    "                concat_str = ''\n",
    "                for str in aas[1].contents:\n",
    "                    if str != \"<br>\" or str != \"<br/>\":\n",
    "                        concat_str = (concat_str + ' '+ str).encode(\"utf-8\").strip()\n",
    "                entry.append(concat_str)\n",
    "\n",
    "                concat_str = ''\n",
    "                for str in times[0].contents:\n",
    "                    if str != \"<br>\" or str != \"<br/>\":\n",
    "                        concat_str = (concat_str + ' '+ str).encode('iso-8859-1').strip()\n",
    "                entry.append(concat_str)\n",
    "\n",
    "                for div_message in div.find_all('div'):\n",
    "                    try:\n",
    "                        class_attribute_message = div_message.get('class')\n",
    "                        if class_attribute_message is not None and class_attribute_message[0] == 'Message':\n",
    "                            blockquotes = []\n",
    "                            x = div_message.get_text()\n",
    "                            for bl in div_message.find_all('blockquote'):\n",
    "                                blockquotes.append(bl.get_text())\n",
    "                                bl.decompose()\n",
    "\n",
    "                            ascii_encoding = div_message.get_text().replace(\"\\n\",\" \").replace(\"<br/>\",\"\").encode('ascii','replace')\n",
    "                            latin1_encoding = ascii_encoding.decode('ascii').encode('iso-8859-1')\n",
    "                            entry.append(latin1_encoding)\n",
    "\n",
    "                            for bl in blockquotes:\n",
    "                                ascii_encoding = bl.replace(\"\\n\",\" \").replace(\"<br/>\",\"\").encode('ascii','replace')\n",
    "                                latin1_encoding = ascii_encoding.decode('ascii').encode('iso-8859-1')\n",
    "                                entry.append(latin1_encoding)\n",
    "                    except Exception as e_message:\n",
    "                        continue\n",
    "\n",
    "                entries.append(entry)\n",
    "        except Exception as e_div:\n",
    "            continue\n",
    "\n",
    "            \n",
    "    urlnumber += 1\n",
    "\n",
    "columns = ['User Name', 'Comment Date', 'Full Comment']\n",
    "\n",
    "# Convert a list of byte to list a of string     \n",
    "stringlist=[[x.decode('iso-8859-1') for x in entry] for entry in entries]\n",
    "# Save the list to a csv file\n",
    "with open('AllComments.csv', 'w') as output:\n",
    "    writer = csv.writer(output, quoting=csv.QUOTE_ALL)\n",
    "    writer.writerow(columns)\n",
    "    writer.writerows(stringlist)\n",
    "\n",
    "print (\"Wrote to AllComments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d81f76-9239-4e93-9231-fba48c67f311",
   "metadata": {},
   "source": [
    "**Note that the comments are downloaded on Jan. 15th, 2024.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
